# Amazon Product Scraper

ğŸ›’ **A complete application to extract Amazon product information using web scraping**

This project consists of a backend API built with **Bun** and **Express** that scrapes Amazon, and a responsive frontend developed with **HTML, CSS, and vanilla JavaScript** using **Vite**.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Technologies Used](#technologies-used)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [How to Run](#how-to-run)
- [API Usage](#api-usage)
- [Project Structure](#project-structure)
- [Functionality](#functionality)
- [Troubleshooting](#troubleshooting)
- [Legal Considerations](#legal-considerations)
- [Contributing](#contributing)

## âœ¨ Features

- ğŸ” **Smart Scraping**: Extracts product title, rating, number of reviews, image, and price
- ğŸ¨ **Modern Interface**: Responsive design inspired by Amazon
- âš¡ **Performance**: Backend optimized with Bun and frontend with Vite
- ğŸ›¡ï¸ **Error Handling**: Robust error handling system
- ğŸ“± **Responsive**: Works perfectly on desktop and mobile
- ğŸ”„ **Loading States**: Visual loading indicators
- ğŸ¯ **Validation**: Real-time input validation

## ğŸ› ï¸ Technologies Used

### Backend
- **[Bun](https://bun.sh/)** - Ultra-fast JavaScript runtime
- **[Express.js](https://expressjs.com/)** - Minimalist web framework
- **[Axios](https://axios-http.com/)** - HTTP client for requests
- **[JSDOM](https://github.com/jsdom/jsdom)** - DOM implementation for Node.js
- **[CORS](https://github.com/expressjs/cors)** - Middleware for Cross-Origin Resource Sharing

### Frontend
- **HTML5** - Semantic structure
- **CSS3** - Modern styling with Flexbox and Grid
- **JavaScript ES6+** - Application logic
- **[Vite](https://vitejs.dev/)** - Build tool and dev server
- **[Font Awesome](https://fontawesome.com/)** - Icons

## ğŸ“¦ Prerequisites

Before you start, make sure you have installed:

- **[Bun](https://bun.sh/)** - Version 1.0.0 or higher
- **[Node.js](https://nodejs.org/)** - Version 18.0.0 or higher (for Vite)
- **[Git](https://git-scm.com/)** - To clone the repository

### Installing Bun

```bash
# On Windows (PowerShell)
irm bun.sh/install.ps1 | iex

# On macOS/Linux
curl -fsSL https://bun.sh/install | bash
```

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/amazon-scraper.git
cd amazon-scraper
```

### 2. Set up the Backend

```bash
cd backend
bun install
```

### 3. Set up the Frontend

```bash
cd ../frontend
npm install
```

## â–¶ï¸ How to Run

### 1. Start the Backend (Terminal 1)

```bash
cd backend
bun run dev
```

The server will be available at: `http://localhost:3000`

### 2. Start the Frontend (Terminal 2)

```bash
cd frontend
npm run dev
```

The frontend will be available at: `http://localhost:5173`

### 3. Access the Application

Open your browser and go to `http://localhost:5173`

## ğŸ”— API Usage

### Available Endpoints

#### 1. Product Scraping
```http
GET /api/scrape?keyword=search_term
```

**Parameters:**
- `keyword` (string, required): Search term for products

**Example Response:**
```json
{
  "success": true,
  "keyword": "smartphone",
  "totalProducts": 16,
  "timestamp": "2025-08-07T10:30:00.000Z",
  "products": [
    {
      "id": 1,
      "title": "Samsung Galaxy S23 Ultra 5G",
      "rating": 4.5,
      "reviewCount": "1,234",
      "imageUrl": "https://m.media-amazon.com/images/I/...",
      "price": "$1,199.99"
    }
  ]
}
```

#### 2. Health Check
```http
GET /api/health
```

#### 3. API Information
```http
GET /
```

### API Usage Examples

```bash
# Search for smartphones
curl "http://localhost:3000/api/scrape?keyword=smartphone"

# Search for laptops
curl "http://localhost:3000/api/scrape?keyword=notebook"

# Health check
curl "http://localhost:3000/api/health"
```

## ğŸ“ Project Structure

```
amazon-scraper/
â”œâ”€â”€ backend/                    # API Backend
â”‚   â”œâ”€â”€ package.json           # Backend dependencies
â”‚   â”œâ”€â”€ server.js              # Main Express server
â”‚   â””â”€â”€ scraper.js             # Web scraping logic
â”œâ”€â”€ frontend/                   # Frontend interface
â”‚   â”œâ”€â”€ package.json           # Frontend dependencies
â”‚   â”œâ”€â”€ index.html             # Main page
â”‚   â”œâ”€â”€ style.css              # CSS styles
â”‚   â””â”€â”€ script.js              # Application JavaScript
â””â”€â”€ README.md                   # This file
```

## ğŸ¯ Functionality

### Backend Features
- âœ… Robust scraping with multiple CSS selectors
- âœ… Custom headers to avoid bot detection
- âœ… Configurable request timeout
- âœ… Comprehensive error handling
- âœ… Well-documented RESTful API
- âœ… Detailed logs for debugging
- âœ… CORS enabled for frontend

### Frontend Features
- âœ… Intuitive and responsive interface
- âœ… Real-time input validation
- âœ… Animated loading states
- âœ… Visual error handling
- âœ… Responsive product grid
- âœ… Star rating system
- âœ… Lazy image loading
- âœ… Automatic retry on errors

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. "Server is not running"
**Solution:**
```bash
# Check if the backend is running
cd backend
bun run dev
```

#### 2. "CORS Error"
**Cause:** Frontend and backend on different ports  
**Solution:** CORS is already configured. Make sure the backend is running on port 3000.

#### 3. "Few products returned"
**Cause:** Amazon may be returning different layouts  
**Solution:** The scraper already has multiple CSS selectors for different layouts.

#### 4. "Bun command not found"
**Solution:**
```bash
# Reinstall Bun
curl -fsSL https://bun.sh/install | bash
source ~/.bashrc  # or ~/.zshrc
```

#### 5. "Timeout Error"
**Cause:** Amazon may be slow or blocking requests  
**Solution:** The timeout is set to 10 seconds. Wait or try again.

### Debug Logs

To enable detailed logs:

```bash
# Backend with detailed logs
cd backend
DEBUG=* bun run dev

# View frontend logs in the browser console
# Open DevTools (F12) > Console
```

### Testing Endpoints

```bash
# Test connectivity
curl http://localhost:3000/api/health

# Simple scraping test
curl "http://localhost:3000/api/scrape?keyword=test"
```

## âš–ï¸ Legal Considerations

> **âš ï¸ IMPORTANT:** This project is for educational and demonstration purposes only.

- ğŸ“– **Educational Use**: Intended for learning web scraping and web development
- ğŸ¤– **Robots.txt**: Always respect the site's robots.txt file
- ğŸš« **Rate Limiting**: Do not make excessive requests that may overload servers
- ğŸ“œ **Terms of Service**: Read and respect Amazon's terms of service
- ğŸ›¡ï¸ **Responsibility**: Use responsibly and ethically

### Best Practices Implemented

- âœ… Headers that simulate real browsers
- âœ… Request timeout to avoid overload
- âœ… Gentle error handling
- âœ… No storage of personal data
- âœ… Focus only on public information

## ğŸ¤ Contributing

Contributions are welcome! Follow these steps:

### 1. Fork the Project
```bash
gh repo fork https://github.com/your-username/amazon-scraper.git
```

### 2. Create a Branch
```bash
git checkout -b feature/new-feature
```

### 3. Commit Your Changes
```bash
git commit -m "feat: add new feature"
```

### 4. Push to Branch
```bash
git push origin feature/new-feature
```

### 5. Open a Pull Request
Create a Pull Request detailing your changes.

### Contribution Guidelines

- ğŸ“ Follow existing code standards
- âœ… Add tests when necessary
- ğŸ“š Update documentation
- ğŸ› Report bugs with details
- ğŸ’¡ Suggest improvements via Issues

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## ğŸ“ Support

- ğŸ› **Bugs**: Open an [Issue](https://github.com/your-username/amazon-scraper/issues)
- ğŸ’¡ **Suggestions**: Use [Discussions](https://github.com/your-username/amazon-scraper/discussions)
- ğŸ“§ **Email**: your-email@example.com

---

**Developed with â¤ï¸ for educational purposes**

â­ If this project helped you, consider giving it a star on GitHub!
